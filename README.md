Implemented multi-layer neural network WITHOUT using external deep learning libraries such as Keras, Caffe, Theano, TensorFlow, PyTorch
 
- The width of the layer 1 is 2, and the width of the layer 2 is 1.
- The activation functions of the layer 1 are the hyperbolic tangent.
- The activation function of the layer 2 is the sigmoid.
- The loss function is the mean squared error.

![Capture](https://github.com/patelvedant312/AI_Playground/assets/45704775/4c775357-5b05-42c4-91a2-d5db4bfd6097)

Optimized parameters using Gradient descent method.
![Capture 1](https://github.com/patelvedant312/AI_Playground/assets/45704775/30182503-1cf5-452d-adb6-153cb5e77c75)

![Capture 2](https://github.com/patelvedant312/AI_Playground/assets/45704775/1d4fb887-3be9-4da9-b5fb-11a03234afdc)
